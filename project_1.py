# -*- coding: utf-8 -*-
"""Project 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hsasEnBBLuC1SOEspCeCEHbdrJfGwDal
"""

import requests
import pandas as pd
from datetime import datetime
from google.colab import files

# Set up GitHub token for authenticated requests to avoid rate limits
GITHUB_TOKEN = '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'  # Replace with your actual GitHub token
HEADERS = {'Authorization': f'token {GITHUB_TOKEN}'} if GITHUB_TOKEN else {}

# Constants
CITY = 'Hyderabad'
# Use a single range to get all users with more than 50 followers
FOLLOWER_MIN = 50

# Helper function for company name cleaning
def clean_company_name(company):
    if company:
        company = company.strip()
        if company.startswith('@'):
            company = company[1:]
        return company.upper()
    return ''

# Step 1: Retrieve users in the specified city with more than 50 followers
def get_users(city, follower_min):
    page = 1
    users = []
    while True:
        url = f"https://api.github.com/search/users?q=location:{city}+followers:>{follower_min}&per_page=100&page={page}"
        response = requests.get(url, headers=HEADERS)
        if response.status_code != 200:
            break  # Stop if there is an error with the request
        data = response.json().get('items', [])
        if not data:
            break
        users.extend(data)
        if len(data) < 100:  # Exit loop if fewer than 100 results
            break
        page += 1
    return users

# Step 2: Gather detailed user data and repositories
def get_user_details(user_login):
    url = f"https://api.github.com/users/{user_login}"
    response = requests.get(url, headers=HEADERS)
    return response.json()

def get_user_repositories(user_login):
    repos = []
    page = 1
    while True:
        url = f"https://api.github.com/users/{user_login}/repos?sort=pushed&per_page=100&page={page}"
        response = requests.get(url, headers=HEADERS)
        data = response.json()
        if not data:
            break
        repos.extend(data)
        if len(repos) >= 500:
            break  # Limit to 500 repositories per user
        page += 1
    return repos[:500]

# Initialize data storage
user_data = []
repo_data = []
unique_users = set()

# Collect all users with more than 50 followers
users = get_users(CITY, FOLLOWER_MIN)
for user in users:
    if user['login'] not in unique_users:  # Ensure uniqueness
        unique_users.add(user['login'])
        user_detail = get_user_details(user['login'])
        user_data.append({
            'login': user_detail.get('login', ''),
            'name': user_detail.get('name', ''),
            'company': clean_company_name(user_detail.get('company', '')),
            'location': user_detail.get('location', ''),
            'email': user_detail.get('email', ''),
            'hireable': user_detail.get('hireable', ''),
            'bio': user_detail.get('bio', ''),
            'public_repos': user_detail.get('public_repos', 0),
            'followers': user_detail.get('followers', 0),
            'following': user_detail.get('following', 0),
            'created_at': user_detail.get('created_at', '')
        })

        # Collect repository data for each user
        repos = get_user_repositories(user['login'])
        for repo in repos:
            repo_data.append({
                'login': user['login'],
                'full_name': repo.get('full_name', ''),
                'created_at': repo.get('created_at', ''),
                'stargazers_count': repo.get('stargazers_count', 0),
                'watchers_count': repo.get('watchers_count', 0),
                'language': repo.get('language', ''),
                'has_projects': repo.get('has_projects', False),
                'has_wiki': repo.get('has_wiki', False),
                'license_name': repo.get('license', {}).get('key') if repo.get('license') else ''
            })

# Step 3: Save data to CSV files
users_df = pd.DataFrame(user_data)
repos_df = pd.DataFrame(repo_data)

users_df.to_csv('users.csv', index=False)
repos_df.to_csv('repositories.csv', index=False)

# Step 4: Create README.md
readme_content = """
* This script uses the GitHub API to collect user and repository data for developers in Hyderabad.
* The data includes users with more than 50 followers and their public repositories.
* Developers are encouraged to engage with open-source projects to enhance their visibility.
"""
with open("README.md", "w") as f:
    f.write(readme_content)

# Step 5: Download files
files.download("users.csv")
files.download("repositories.csv")
files.download("README.md")

import pandas as pd
from google.colab import files

# This will prompt you to upload your CSV files
uploaded = files.upload()

# Load the repositories data
repositories_df = pd.read_csv('repositories.csv')

# Load the users data
users_df = pd.read_csv('users.csv')

# Convert 'created_at' to datetime
users_df['created_at'] = pd.to_datetime(users_df['created_at'])

# Filter users who joined after 2020
recent_users = users_df[users_df['created_at'] > '2020-01-01']

# Merge with repositories to find languages used by these users
recent_repos = repositories_df[repositories_df['login'].isin(recent_users['login'])]

# Count the occurrences of each programming language
language_counts = recent_repos['language'].value_counts()

# Get the second most popular language
second_most_popular_language = language_counts.index[1] if len(language_counts) > 1 else None

# Display the result
print("The second most popular programming language among users who joined after 2020 is:", second_most_popular_language)

import pandas as pd

# Load the repositories data
repositories_df = pd.read_csv('repositories.csv')

# Check if 'language' and 'stargazers_count' columns exist
if 'language' not in repositories_df.columns or 'stargazers_count' not in repositories_df.columns:
    raise ValueError("The required columns 'language' or 'stargazers_count' are missing from the dataset.")

# Group by programming language and calculate the average number of stars
average_stars_per_language = repositories_df.groupby('language')['stargazers_count'].mean()

# Find the language with the highest average number of stars
highest_avg_stars_language = average_stars_per_language.idxmax()
highest_avg_stars_value = average_stars_per_language.max()

# Display the result
print(f"The programming language with the highest average number of stars per repository is: {highest_avg_stars_language} with an average of {highest_avg_stars_value:.2f} stars.")

import pandas as pd

# Load the users data
users_df = pd.read_csv('users.csv')

# Check if 'followers' and 'following' columns exist
if 'followers' not in users_df.columns or 'following' not in users_df.columns:
    raise ValueError("The required columns 'followers' or 'following' are missing from the dataset.")

# Calculate leader strength
users_df['leader_strength'] = users_df['followers'] / (1 + users_df['following'])

# Get the top 5 users by leader strength
top_leaders = users_df.nlargest(5, 'leader_strength')

# Extract their logins in order
top_leader_logins = ', '.join(top_leaders['login'].tolist())

# Display the result
print("Top 5 users by leader strength (login in order):", top_leader_logins)

import pandas as pd

# Load the users data
users_df = pd.read_csv('users.csv')

# Filter for users in Hyderabad
hyderabad_users = users_df[users_df['location'].str.contains('Hyderabad', case=False, na=False)]

# Check if 'followers' and 'public_repos' columns exist
if 'followers' not in hyderabad_users.columns or 'public_repos' not in hyderabad_users.columns:
    raise ValueError("The required columns 'followers' or 'public_repos' are missing from the dataset.")

# Calculate the correlation
correlation = hyderabad_users['followers'].corr(hyderabad_users['public_repos'])

# Display the result rounded to 3 decimal places
print(f"Correlation between followers and public repositories among users in Hyderabad: {correlation:.3f}")

import pandas as pd
import statsmodels.api as sm

# Load the users data
users_df = pd.read_csv('users.csv')

# Check if 'followers' and 'public_repos' columns exist
if 'followers' not in users_df.columns or 'public_repos' not in users_df.columns:
    raise ValueError("The required columns 'followers' or 'public_repos' are missing from the dataset.")

# Define the independent (X) and dependent (y) variables
X = users_df['public_repos']
y = users_df['followers']

# Add a constant to the independent variable (for the intercept)
X = sm.add_constant(X)

# Perform linear regression
model = sm.OLS(y, X).fit()

# Get the slope for public_repos (the coefficient for the second variable)
slope = model.params['public_repos']

# Display the result rounded to 3 decimal places
print(f"Regression slope of followers on public repositories: {slope:.3f}")

import pandas as pd

# Load the repositories data
repositories_df = pd.read_csv('repositories.csv')

# Check if 'has_projects' and 'has_wiki' columns exist
if 'has_projects' not in repositories_df.columns or 'has_wiki' not in repositories_df.columns:
    raise ValueError("The required columns 'has_projects' or 'has_wiki' are missing from the dataset.")

# Convert boolean columns to integers (1 for True, 0 for False)
repositories_df['has_projects'] = repositories_df['has_projects'].astype(int)
repositories_df['has_wiki'] = repositories_df['has_wiki'].astype(int)

# Calculate the correlation
correlation = repositories_df['has_projects'].corr(repositories_df['has_wiki'])

# Display the result rounded to 3 decimal places
print(f"Correlation between projects enabled and wiki enabled: {correlation:.3f}")

import pandas as pd

# Load the users data
users_df = pd.read_csv('users.csv')

# Check if 'following' and 'hireable' columns exist
if 'following' not in users_df.columns or 'hireable' not in users_df.columns:
    raise ValueError("The required columns 'following' or 'hireable' are missing from the dataset.")

# Replace blank values in 'hireable' with False
users_df['hireable'] = users_df['hireable'].replace('', False)

# Convert 'hireable' to boolean, treating any non-True value (including empty strings) as False
users_df['hireable'] = users_df['hireable'].map(lambda x: True if str(x).lower() == 'true' else False)

# Clean the data: Remove rows where 'following' is NaN
users_df = users_df.dropna(subset=['following'])

# Calculate average following for hireable users (where hireable is True)
hireable_avg_following = users_df[users_df['hireable'] == True]['following'].mean()

# Calculate average following for non-hireable users (where hireable is False)
non_hireable_avg_following = users_df[users_df['hireable'] == False]['following'].mean()

# Calculate the difference
difference = hireable_avg_following - non_hireable_avg_following

# Display the result rounded to 3 decimal places
print(f"Average following per user for hireable users minus the average following for the rest: {difference:.3f}")

from sklearn.linear_model import LinearRegression
import numpy as np

# Clean the data: Remove rows where 'followers' or 'bio' is NaN
users_df = users_df.dropna(subset=['followers', 'bio'])

# Calculate the word count for bios
users_df['bio_word_count'] = users_df['bio'].apply(lambda x: len(x.split()))

# Prepare the data for regression
X = users_df[['bio_word_count']]
y = users_df['followers']

# Fit the regression model
model = LinearRegression()
model.fit(X, y)

# Get the regression slope (coefficient)
slope = model.coef_[0]

# Display the result rounded to 3 decimal places
print(f"Regression slope of followers on bio word count: {slope:.3f}")

# Ensure 'created_at' is a datetime column
repositories_df['created_at'] = pd.to_datetime(repositories_df['created_at'])

# Filter for weekend days (Saturday = 5, Sunday = 6)
weekend_repos = repositories_df[repositories_df['created_at'].dt.dayofweek >= 5]

# Count repositories created by each user
top_users = weekend_repos['login'].value_counts().head(5)

# Display the top 5 users' login in order, comma-separated
top_user_logins = ', '.join(top_users.index)
print(f"Top 5 users who created the most repositories on weekends: {top_user_logins}")

import pandas as pd

# Load the users data
users_df = pd.read_csv('Tools in Data Science_ Project 1 - Attempt 3.csv')

# Ensure the required columns exist
if 'hireable' not in users_df.columns or 'email' not in users_df.columns:
    raise ValueError("The required columns 'hireable' or 'email' are missing from the dataset.")

# Replace blank values in 'email' with NaN
users_df['email'].replace('', pd.NA, inplace=True)

# Convert 'hireable' to boolean, treating any non-True value (including blanks) as False
users_df['hireable'] = users_df['hireable'].replace('', False)
users_df['hireable'] = users_df['hireable'].map(lambda x: True if str(x).lower() == 'true' else False)

# Calculate fraction of users with emails
total_users = users_df.shape[0]

# Fraction of users with emails for hireable users
hireable_users = users_df[users_df['hireable'] == True]
hireable_email_fraction = hireable_users['email'].notnull().mean()

# Fraction of users with emails for non-hireable users
non_hireable_users = users_df[users_df['hireable'] == False]
non_hireable_email_fraction = non_hireable_users['email'].notnull().mean()

# Calculate the difference
email_difference = hireable_email_fraction - non_hireable_email_fraction

# Display the result rounded to 3 decimal places
print(f"Fraction of users with email when hireable=true minus for the rest: {email_difference:.3f}")

import pandas as pd

# Load the users data
users_df = pd.read_csv('Tools in Data Science_ Project 1 - Attempt 3.csv')

# Ensure the required columns exist
if 'hireable' not in users_df.columns or 'following' not in users_df.columns:
    raise ValueError("The required columns 'hireable' or 'following' are missing from the dataset.")

# Convert 'hireable' to boolean, treating any non-True value (including blanks) as False
users_df['hireable'] = users_df['hireable'].replace('', False)
users_df['hireable'] = users_df['hireable'].map(lambda x: True if str(x).lower() == 'true' else False)

# Convert 'following' to numeric, using errors='coerce' to handle invalid entries
users_df['following'] = pd.to_numeric(users_df['following'], errors='coerce')

# Filter out blanks and NaNs from the 'following' column
users_df = users_df[users_df['following'].notna()]

# Calculate average following for hireable users
hireable_avg_following = users_df[users_df['hireable']]['following'].mean()

# Calculate average following for non-hireable users
non_hireable_avg_following = users_df[~users_df['hireable']]['following'].mean()

# Calculate the difference
following_difference = hireable_avg_following - non_hireable_avg_following

# Display the result rounded to 3 decimal places
print(f"Average following per user for hireable=true minus the average following for the rest: {following_difference:.3f}")

# Clean the data: Remove rows where 'name' is NaN
users_df = users_df.dropna(subset=['name'])

# Extract surnames (last word in names)
users_df['surname'] = users_df['name'].apply(lambda x: x.strip().split()[-1])

# Count the occurrences of each surname
surname_counts = users_df['surname'].value_counts()

# Get the most common surname(s)
most_common_surnames = surname_counts[surname_counts == surname_counts.max()].index.tolist()

# Sort the surnames alphabetically if there's a tie
most_common_surnames.sort()

# Display the result as a comma-separated list
surnames_str = ', '.join(most_common_surnames)
print(f"Most common surname(s): {surnames_str}")

import pandas as pd

# Load the users data
users_df = pd.read_csv('users.csv')

# Ensure 'location' and 'created_at' columns exist
if 'location' not in users_df.columns or 'created_at' not in users_df.columns:
    raise ValueError("The required columns 'location' or 'created_at' are missing from the dataset.")

# Filter for users in Hyderabad
hyderabad_users = users_df[users_df['location'].str.lower() == 'hyderabad']

# Convert 'created_at' to datetime format
hyderabad_users['created_at'] = pd.to_datetime(hyderabad_users['created_at'])

# Sort users by 'created_at' in ascending order and select the earliest 5
earliest_users = hyderabad_users.nsmallest(5, 'created_at')

# Get the logins in ascending order of 'created_at'
earliest_user_logins = ', '.join(earliest_users['login'].tolist())

# Display the result
print(f"The 5 earliest registered GitHub users in Hyderabad: {earliest_user_logins}")

import pandas as pd

# Load the users data
users_df = pd.read_csv('Tools in Data Science_ Project 1 - Attempt 3.csv')

# Ensure the required columns exist
if 'hireable' not in users_df.columns or 'email' not in users_df.columns:
    raise ValueError("The required columns 'hireable' or 'email' are missing from the dataset.")

# Replace blank values in 'email' with NaN
users_df['email'].replace('', pd.NA, inplace=True)

# Convert 'hireable' to boolean, treating any non-True value (including blanks) as False
users_df['hireable'] = users_df['hireable'].replace('', False)
users_df['hireable'] = users_df['hireable'].map(lambda x: True if str(x).lower() == 'true' else False)

# Count total users and users with emails for hireable group
hireable_users = users_df[users_df['hireable']]
total_hireable_users = hireable_users.shape[0]
users_with_email_hireable = hireable_users['email'].notnull().sum()

# Count total users and users with emails for non-hireable group
non_hireable_users = users_df[~users_df['hireable']]
total_non_hireable_users = non_hireable_users.shape[0]
users_with_email_non_hireable = non_hireable_users['email'].notnull().sum()

# Calculate fractions
hireable_email_fraction = users_with_email_hireable / total_hireable_users if total_hireable_users > 0 else 0
non_hireable_email_fraction = users_with_email_non_hireable / total_non_hireable_users if total_non_hireable_users > 0 else 0

# Calculate the difference
email_difference = hireable_email_fraction - non_hireable_email_fraction

# Display the result rounded to 3 decimal places
print(f"Fraction of users with email when hireable=true minus for the rest: {email_difference:.3f}")